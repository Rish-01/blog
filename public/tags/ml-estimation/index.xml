<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML Estimation on Rish&#39;s AI Notes</title>
    <link>http://localhost:1313/blog/tags/ml-estimation/</link>
    <description>Recent content in ML Estimation on Rish&#39;s AI Notes</description>
    <image>
      <title>Rish&#39;s AI Notes</title>
      <url>http://localhost:1313/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.139.4</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Mar 2025 00:13:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/tags/ml-estimation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational Autoencoders and Maximum Likelihood Estimation</title>
      <link>http://localhost:1313/blog/posts/vae/</link>
      <pubDate>Thu, 06 Mar 2025 00:13:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/posts/vae/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://rish-01.github.io/blog/posts/ml_estimation/&#34;&gt;previous blog&lt;/a&gt;, we explored maximum likelihood estimation (MLE) and how it can be used to derive commonly used loss functions. It also turns out that MLE is widely being used in generative models like Variational Autoencoders (VAE) and Diffusion models (DDPM).&lt;/p&gt;
&lt;p&gt;In this blog, we will explore how the loss function of Variational Autoencoders are derived. VAEs are latent variable generative models. They can solve a few tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Act as a generative model that mimics the data distribution that it was trained on.&lt;/li&gt;
&lt;li&gt;Approximate posterior inference of the latent variable $z$ given an observed variable $x$. In other words, it can be used to learn lower dimensional representations of the data it was trained on.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;preliminary-information&#34;&gt;Preliminary Information&lt;/h2&gt;
&lt;p&gt;In this section, let us explore the tools necessary to derive a tractable form of the log-likelihood that we need to optimize.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation and Loss Functions</title>
      <link>http://localhost:1313/blog/posts/ml_estimation/</link>
      <pubDate>Sun, 15 Dec 2024 01:06:38 +0530</pubDate>
      <guid>http://localhost:1313/blog/posts/ml_estimation/</guid>
      <description>&lt;p&gt;When I started learning about loss functions, I could always understand the intuition behind them. For example, the mean squared error (MSE) for regression seemed logical—penalizing large deviations from the ground-truth makes sense. But one thing always bothered me: I could never come up with those loss functions on my own. Where did they come from? Why do we use these specific formulas and not something else?&lt;/p&gt;
&lt;p&gt;This frustration led me to dig deeper into the mathematical and probabilistic foundations of loss functions. It turns out, the answers lie in a concept called Maximum Likelihood Estimation (MLE). In this blog, I’ll take you through this journey, showing how these loss functions are not arbitrary but derive naturally from statistical principles. I&amp;rsquo;ll start by defining what Maximum Likelihood Estimation (MLE) is, followed by the intricate connection between Maximum Likelihood Estimation (MLE) and Kullback-Leibler (KL) divergence. To conclude this article, I show how loss functions like Mean Squared Error loss and Binary Cross Entropy can be derived from Maximum Likelihood estimation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
